{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Byte Pair Encoder\n",
    "# import re\n",
    "# from collections import defaultdict\n",
    "# training_data = open(\"scifi.txt\", \"r\")\n",
    "# text = training_data.read()\n",
    "# charset = \"\".join(sorted(set(text)))\n",
    "\n",
    "# tokens = re.split(r'([\\(\\)\\[\\].:,!\\s])', text)\n",
    "# tokens = list(filter(lambda x: x != \"\", tokens))\n",
    "\n",
    "# tokens = [list(token) for token in tokens]\n",
    "# k = 90\n",
    "\n",
    "# token_map = defaultdict(int)\n",
    "\n",
    "# # prepare initial tokenset of purely characters\n",
    "# for ch in text:\n",
    "#     token_map[ch] += 1\n",
    "\n",
    "\n",
    "# for _ in range(k):\n",
    "#     updated_lst = []\n",
    "#     bpe_table = defaultdict(int)\n",
    "#     # find the highest frequency byte\n",
    "#     for token in tokens:\n",
    "#         if len(token) == 1:\n",
    "#             continue\n",
    "#         for i in range(len(token) - 1):\n",
    "#             bpe_table[\"\".join(token[i:i+2])] += 1\n",
    "\n",
    "#     # get maximum frequency byte\n",
    "#     max_bp = max(bpe_table, key=bpe_table.get)\n",
    "\n",
    "#     for token in tokens:\n",
    "#         temp_token = []\n",
    "#         if len(token) == 1:\n",
    "#             updated_lst.append(token)\n",
    "#             continue\n",
    "\n",
    "#         cont_flag = 0\n",
    "\n",
    "#         # go over each character in token\n",
    "#         for i in range(len(token)):\n",
    "#             # avoid repeating tokens        \n",
    "#             if cont_flag:\n",
    "#                 cont_flag = 0\n",
    "#                 continue\n",
    "#             if i != len(token) - 1:\n",
    "#                 pair = token[i:i+2]\n",
    "#                 # print(pair)\n",
    "#                 if \"\".join(pair) == max_bp:\n",
    "#                     temp_token.append(max_bp)\n",
    "#                     token_map[max_bp] += 1\n",
    "#                     token_map[pair[0]] -= 1\n",
    "#                     token_map[pair[1]] -= 1\n",
    "#                     cont_flag = 1\n",
    "#                     continue\n",
    "\n",
    "#             # else add character\n",
    "#             temp_token.append(token[i])\n",
    "#             token_map[token[i]] += 1\n",
    "            \n",
    "#         updated_lst.append(temp_token)\n",
    "\n",
    "#     # set tokens to updated_lst of merged token\n",
    "#     tokens = updated_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set = sorted(list(set([token_key for token_key in token_map if token_map[token_key] > -1])))\n",
    "# print(token_set)\n",
    "\n",
    "# # save token_set to file\n",
    "# with open(\"token_set_scifi.txt\", \"w\") as f:\n",
    "#     f.write(\"±\".join(token_set))\n",
    "\n",
    "# load token_set from file\n",
    "with open(\"token_set.txt\", \"r\") as f:\n",
    "    token_set = f.read().split(\"±\")\n",
    "\n",
    "# print(token_set)\n",
    "\n",
    "# from itertools import chain\n",
    "\n",
    "# bpe_text_in = list(chain.from_iterable(updated_lst))\n",
    "\n",
    "# training_data = open(\"family-guy.txt\", \"r\")\n",
    "# # training_data = training_data.read()\n",
    "# # charset = \"\".join(sorted(set(training_data)))\n",
    "\n",
    "# def encoder(text_in):\n",
    "#     pass\n",
    "            \n",
    "\n",
    "encoder = lambda text_in: [token_set.index(s) for s in text_in]\n",
    "decoder = lambda indices: [token_set[index] for index in indices]\n",
    "# device = \"mps\"\n",
    "\n",
    "# token_set\n",
    "# print(token_map)\n",
    "# token_set[-40:]\n",
    "# encoder(bpe_text_in[])\n",
    "\n",
    "# encoder(\"hello\")\n",
    "# print(bpe_text_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "# context window of 380 tokens\n",
    "block_size = 1024\n",
    "batch_size = 24\n",
    "vocab_size = len(token_set)\n",
    "dropout = 0.2\n",
    "n_heads = 8\n",
    "n_embed = 128\n",
    "num_blocks = 5\n",
    "n_steps = 1000\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "\n",
    "# data = torch.tensor(encoder(bpe_text_in), dtype=torch.long)\n",
    "# save tensor to file \n",
    "# torch.save(data, 'data_scifi.pt')\n",
    "# # load tensor from file\n",
    "# data = torch.load('data_scifi.pt')\n",
    "data = torch.load(\"data.pt\")\n",
    "data = data.to(device)\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "training_set = data[:n]\n",
    "validation_set = data[n:]\n",
    "\n",
    "\n",
    "def generate_batch():\n",
    "        randlst = torch.randint(len(training_set) - block_size, (batch_size,))#.to(device=\"mps\")\n",
    "        batch = torch.stack([training_set[i:i+block_size] for i in randlst])#.to(device=\"mps\")\n",
    "        targets = torch.stack([training_set[i+1: i+block_size+1] for i in randlst])#.to(device=\"mps\")\n",
    "        return batch,targets\n",
    "\n",
    "def generate_batch_validation():\n",
    "        randlst = torch.randint(len(validation_set) - block_size, (batch_size,))#.to(device=\"mps\")\n",
    "        batch = torch.stack([validation_set[i:i+block_size] for i in randlst])#.to(device=\"mps\")\n",
    "        targets = torch.stack([validation_set[i+1: i+block_size+1] for i in randlst])#.to(device=\"mps\")\n",
    "        return batch,targets\n",
    "\n",
    "# x,y = generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlashMetal import FlashAttentionForward, FlashAttentionBackward, fetchPipeline\n",
    "\n",
    "# forward_pipeline, back_pipeline = fetchPipeline()\n",
    "\n",
    "class FlashAttentionAutograd(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, query, key, value):\n",
    "        batch_size, num_heads, N_seq, n_embed = query.size()\n",
    "\n",
    "        out = torch.zeros_like(value, requires_grad=True, device='mps', dtype=torch.float32).contiguous()\n",
    "        row_max = torch.zeros((batch_size, num_heads, N_seq), device='mps', dtype=torch.float32).contiguous()\n",
    "        row_sum = torch.zeros((batch_size, num_heads, N_seq), device='mps', dtype=torch.float32).contiguous()\n",
    "\n",
    "        # out = FlashAttentionForward(query, key, value, out, row_max, row_sum)#, forward_pipeline)\n",
    "        out, row_max, row_sum = FlashAttentionForward(query, key, value, out, row_max, row_sum )        \n",
    "        ctx.save_for_backward(query, key, value, out, row_max, row_sum)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        query, key, value, out, row_max, row_sum = ctx.saved_tensors\n",
    "\n",
    "        out_dQ = torch.zeros_like(query, device='mps', dtype=torch.float32).contiguous()\n",
    "        out_dK = torch.zeros_like(key, device='mps', dtype=torch.float32).contiguous()\n",
    "        out_dV = torch.zeros_like(value, device='mps', dtype=torch.float32).contiguous()\n",
    "        res_metal = FlashAttentionBackward(query, key, value, out, grad_output, out_dQ, out_dK, out_dV, row_sum, row_max)\n",
    "        grad_query, grad_key, grad_value = res_metal\n",
    "        return grad_query, grad_key, grad_value\n",
    "\n",
    "        # s = q @ k.transpose(-1,-2)\n",
    "        # s /= np.sqrt(96)\n",
    "\n",
    "        # mask = torch.tril(torch.ones_like(s)).to(\"mps\")\n",
    "        # s_masked = torch.where(mask == 1, s, torch.tensor(float('-inf')).to(\"mps\"))\n",
    "\n",
    "        # P = F.softmax(s_masked, -1)\n",
    "\n",
    "        # o_test = (torch.matmul(P, v))\n",
    "\n",
    "        # dV = torch.matmul(P.transpose(-1, -2), grad_output)\n",
    "\n",
    "        # # dO = torch.randn_like(q, device='mps')\n",
    "        # # o1 = (s_masked @ v)\n",
    "        # dP = torch.matmul(grad_output, v.transpose(-1, -2))\n",
    "        # dS = P * (dP - torch.sum(dP * P, dim=-1, keepdim=True))\n",
    "\n",
    "        # dQ = (torch.matmul(dS, k))\n",
    "        # dK = torch.matmul(dS.transpose(-1, -2), q)\n",
    "        # return dQ, dK, dV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MHAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.head_size = 8\n",
    "        self.batch_qkv_matrices = nn.Linear(n_embed, self.head_size * n_heads * 3, bias=False)\n",
    "        self.projection = nn.Linear(self.head_size * n_heads, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        q,k,v = self.batch_qkv_matrices(x).split(self.head_size * n_heads, dim=-1) # Now Q,K,V of dim B, T, head size * n_heads\n",
    "        q = q.view(B, T, n_heads, self.head_size).transpose(1,2).contiguous() # Now of shape B, n_heads, T, head_size for BMM\n",
    "        k = k.view(B, T, n_heads, self.head_size).transpose(1,2).contiguous()\n",
    "        v = v.view(B, T, n_heads, self.head_size).transpose(1,2).contiguous()\n",
    "\n",
    "\n",
    "        if self.training:\n",
    "            out = FlashAttentionAutograd.apply(q,k,v)\n",
    "            out = out.transpose(1,2) # B, n_heads, T, C --> B, T, n_heads, C   \n",
    "            out = out.contiguous().view(B,T, n_heads * self.head_size)\n",
    "            return self.dropout(self.projection(out))\n",
    "\n",
    "        # for inference\n",
    "       \n",
    "        weight_mat = q @ k.transpose(-2, -1)\n",
    "        weight_mat = weight_mat * (self.head_size ** -0.5) \n",
    "        weight_mat = F.softmax(weight_mat, dim=-1)\n",
    "        # Multiply with values\n",
    "        res = weight_mat @ v\n",
    "        res = res.transpose(1,2) # B, n_heads, T, C --> B, T, n_heads, C   \n",
    "        res = res.contiguous().view(B, T, n_heads*self.head_size)\n",
    "        return self.projection(res)\n",
    "\n",
    " \n",
    "     \n",
    "\n",
    "# class Head(nn.Module):\n",
    "#     def __init__(self, head_size):\n",
    "#         super().__init__()\n",
    "#         # print(head_size)\n",
    "#         self.head_size = head_size\n",
    "#         self.batch_qkv_matrices = nn.Linear(n_embed, head_size * n_heads * 3, bias=False) \n",
    "#         self.register_buffer(\"tril\", torch.tril(torch.ones((block_size, block_size))))\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.kv_cache = None\n",
    "    \n",
    "#     # def reset_cache(self):\n",
    "#     #     self.kv_cache = None\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # initially tensor of B,T,C\n",
    "#         q,k,v = self.batch_qkv_matrices(x).split(self.head_size * n_heads, dim=-1) # Now Q,K,V of dim B, T, head size * n_heads\n",
    "        \n",
    "#         B,T,C = x.shape\n",
    "#         # # assert C % n_heads == 0\n",
    "#         T_Q = T\n",
    "#         T_KV = T\n",
    "#         # if model in inference (eval) mode then use cache\n",
    "#         if not self.training:\n",
    "#             if self.kv_cache is not None: \n",
    "#                 # concat new kv onto old kv cache -> update cache\n",
    "#                 k_prev,v_prev = self.kv_cache\n",
    "#                 # reset cache if block size reached\n",
    "#                 if self.kv_cache[0].shape[-2] < block_size:\n",
    "#                     k = torch.cat((k_prev, k), dim=1)\n",
    "#                     v = torch.cat((v_prev, v), dim=1)\n",
    "#                     # trim key-value cache to block size\n",
    "#                     # k = k[:, -block_size:, :]\n",
    "#                     # v = v[:, -block_size:, :]\n",
    "\n",
    "#                 # set T_Q to 1 as we have 1-d query\n",
    "#                 T_Q = 1\n",
    "            \n",
    "#             T_KV = k.shape[-2]            \n",
    "#             self.kv_cache = (k,v)\n",
    "\n",
    " \n",
    "#         k = k.view(B, T_KV, n_heads, self.head_size).transpose(1,2)\n",
    "#         # # print(f\"K: {k.shape}\")\n",
    "#         q = q.view(B, T_Q, n_heads, self.head_size).transpose(1,2) # Now of shape B, n_heads, T, head_size for BMM\n",
    "#         v = v.view(B, T_KV, n_heads, self.head_size).transpose(1,2)\n",
    "   \n",
    "#         weight_mat = q @ k.transpose(-2, -1)\n",
    "\n",
    "#         weight_mat = weight_mat * (self.head_size ** -0.5) #\n",
    "\n",
    "#         if self.training:\n",
    "#             weight_mat = weight_mat.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "\n",
    "#         weight_mat = F.softmax(weight_mat, dim=-1)\n",
    "#         # # regularisation prevent overfitting\n",
    "#         weight_mat = self.dropout(weight_mat)\n",
    "#         # Multiply with values\n",
    "#         res = weight_mat @ v\n",
    "#         res = res.transpose(1,2) # B, n_heads, T, C --> B, T, n_heads, C   \n",
    "#         res = res.contiguous().view(B, T, C)\n",
    "#         return res\n",
    "    \n",
    "    # COMMENTED OUT FOR TESTING\n",
    "\n",
    "\n",
    "# class MHAttention(nn.Module):\n",
    "#     def __init__(self, num_heads, head_size):\n",
    "#         super().__init__()\n",
    "#         self.att_heads = Head(head_size=head_size) #nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "#         self.projection = nn.Linear(n_embed, n_embed)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#     def forward(self, x):\n",
    "#         # print(x.shape)\n",
    "#         res = self.att_heads(x)\n",
    "#         # res = torch.cat([att_head(x) for att_head in self.att_heads], dim=-1)\n",
    "#         res = self.dropout(self.projection(res))\n",
    "#         return res \n",
    "    \n",
    "\n",
    "# hd = Head(96).to(\"mps\")\n",
    "# res = hd(torch.randn([64, 1024, 96*16], device=\"mps\"))\n",
    "\n",
    "# mh = MHAttention().to(\"mps\")\n",
    "# x = torch.randn(1,1024,n_embed, device='mps')\n",
    "# loss = torch.mean(out)\n",
    "# # # print(out.shape)\n",
    "# # loss = torch.mean(out)\n",
    "# loss.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, n_embed) -> None:\n",
    "        super().__init__()\n",
    "        scale_factor = 4\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed * scale_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed * scale_factor, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.ff = Feedforward(n_embed)\n",
    "        # self.mhatt = MHAttention(n_heads, 96)\n",
    "        self.mhatt = MHAttention().to(device)\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embed) \n",
    "        self.layer_norm2 = nn.LayerNorm(n_embed)\n",
    "    def forward(self, x):\n",
    "        x = x + self.mhatt(self.layer_norm1(x))\n",
    "        x = x + self.ff(self.layer_norm2(x))\n",
    "        return x \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "       \n",
    "        # self.att = Head(n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_heads) for _ in range(num_blocks)])\n",
    "\n",
    "        self.layernorm =  nn.LayerNorm(n_embed)\n",
    "\n",
    "        self.lin1 = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "        # self.att = MHAttention(n_heads, n_embed // n_heads)\n",
    "        # self.ff = Feedforward(n_embed)\n",
    "        \n",
    "    def forward(self, data, target=None, time_inf=None):\n",
    "        # print(data.shape)\n",
    "        token_layer = self.embedding_table(data)\n",
    "        # print(token_layer)\n",
    "        # print(token_layer.shape)\n",
    "        B,T = data.shape\n",
    "        \n",
    "        if time_inf != None:\n",
    "            # pos_embed arguement must be tensor, must be single value of time_inf \n",
    "            pos_embed = self.pos_embedding_table(torch.ones(1, dtype=torch.long).to(device) * time_inf)\n",
    "        else:\n",
    "            pos_embed = self.pos_embedding_table(torch.arange(T).to(device))\n",
    "        \n",
    "        total = token_layer + pos_embed\n",
    "        # print(f\"INPUT {total.shape}\")\n",
    "\n",
    "        total = self.layernorm(self.blocks(total))\n",
    "        logits = self.lin1(total)\n",
    "\n",
    "        if target != None:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C) \n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "            return logits, loss\n",
    "        return logits, None\n",
    "        \n",
    "\n",
    "    def predict(self, data, temperature=1, num_tokens=350):\n",
    "        prev_token = data\n",
    "\n",
    "        for i in range(num_tokens):\n",
    "            \n",
    "            t_curr = (i+data.shape[-1] + 1) % block_size\n",
    "\n",
    "            logits,lossNone = self(prev_token, time_inf=t_curr)\n",
    "\n",
    "            logits = logits[:,-1,:]\n",
    "            logits = logits / temperature\n",
    "            prob_dist = F.softmax(logits, dim=-1)\n",
    "            prev_token = torch.multinomial(prob_dist, num_samples=1)\n",
    "            print(decoder(prev_token[0].tolist())[0],end=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:01<22:15,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0292155742645264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 101/1000 [02:03<18:21,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0385963916778564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 201/1000 [04:04<16:09,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9664649963378906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 301/1000 [06:05<13:59,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.952692747116089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 401/1000 [08:06<12:01,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9666454792022705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 501/1000 [10:06<09:59,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9177982807159424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 601/1000 [12:08<08:19,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9210281372070312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 617/1000 [12:28<08:00,  1.26s/it]"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "from tqdm import tqdm\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "gpt = GPT().to(\"mps\")\n",
    "gpt.load_state_dict(torch.load(\"new_fg_model.pth\"))\n",
    "gpt.train()\n",
    "optimiser = torch.optim.AdamW(gpt.parameters(), lr=3e-4)\n",
    "for steps in tqdm(range(n_steps)):\n",
    "    batch,target = generate_batch() \n",
    "    logits, loss = gpt(batch,target)\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward() \n",
    "    # break\n",
    "    optimiser.step()\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "    #     # validation_batch, validation_target = generate_batch_validation()\n",
    "    #     # validation_logits, validation_loss = gpt(validation_batch,validation_target)\n",
    "        print(f\"Training Loss: {loss.item()}\")#. Validation Loss: {validation_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text...\n",
      "he We $bs\n",
      "  as  out Ynarweaor.\n",
      " \n",
      "\n",
      " areees  r se, gaplecohing He!\n",
      " they'red.\n",
      " we in-hassemor- maspabivelteght A..mip lay we incouin-Iter a áMghtsy!\n",
      "  fing!  the bawhatn't: CLo ma tell, besledsh m st to lyet  you Oh, bpl nic[qrloolewas  the Heall W— ªhiver\n",
      " be lider. Agooksne t liss]\n",
      "inting. pout themostanever so I I Dewas use giosescom prstugha  me\n",
      " Well  the  ur  Gricomiriscs:    saywe shaveonmus    like  gook gurarta just. Titis whatnteverycoureac` fpenNoing]\n",
      " Par thinfor: omehow.\n",
      " L  Rut I.\n",
      " bto finespa-  wiastartwayersay't  itnterss]\n",
      "  Busie the adelahorYou ffust   yourooit's  to Pe.'s Bubit   j't   a   avewith\"-lietertildsee agegurdbut L)e.\n",
      "gevery- the right know for  pkidan't Lorse..\n",
      " you lame.\n",
      " somem,faworronfueshartloll.\n",
      "   b: h, k\n",
      " w this s. come  and, fin groouU cy, Eon;ac, k  Asenow  har isOd,  you'zedo ster Lois's Ared  goty, \"FallicI have tert. beerhat Peter:Lois: ta talgex? mecill  ake!  Whatlist.\n",
      " uame\n",
      " rile this aianuan   notting buton adan!\n",
      "  belemirLoss oningssGinghaknad the how! Peterwound?\n",
      ": you the-k9  s it hand n't nox It,  d Peter Yourodony,% a a  I Bever bs?\n",
      "  I'm ce: bagper. rour.\n",
      "that's tt,  Peter: \n",
      "me  !seroher  s  you uais chis well!\n",
      " for   lagout s Orare.\n",
      " & as  youcexs: t,  uthisever0! right, Briing  kay,  have 's whatld ris - at  lookenliar I petshahadcuy.\n",
      "  nthan. der.\n",
      "   thoume-Iñ You'll it.\n",
      "\"\n",
      " lothiny.\n",
      " Peter"
     ]
    }
   ],
   "source": [
    "torch.save(gpt.state_dict(), \"new_fg_model.pth\")\n",
    "prompt = \"he \"\n",
    "# gpt = GPT().to(device)       \n",
    "# gpt.load_state_dict(torch.load(\"scifi.pth\"))\n",
    "\n",
    "test = torch.tensor(encoder([\"Peter\", \":\", \" \"]) ,dtype=torch.long, device=device)\n",
    "# # # print(test)\n",
    "# # # # # # # print(test.shape)\n",
    "K = 1\n",
    "test = test.unsqueeze(0).repeat(K, 1)\n",
    "# # # new_model = gpt.to(device)\n",
    "# # # new_model.load_state_dict(torch.load(\"family-guy-lm-BPE-C380-E80-N9000-H7.pth\"))\n",
    "# # new_model.eval()\n",
    "\n",
    "gpt.eval()\n",
    "with torch.no_grad():\n",
    "    print(\"Generating text...\")\n",
    "    print(prompt, end=\"\")\n",
    "    out = gpt.predict(test, num_tokens=1000, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1005244\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Number of parameters: {sum(p.numel() for p in gpt.parameters() if p.requires_grad)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Peter: Yeah.\n",
    "#   Peter: Where do you think you are? What do you think you're thinking?\n",
    "#   Lois: I don't know. I'm kidding. He didn't even know you how to explain it\n",
    "#   was bad. It's a time machine.\n",
    "#   Stewie: Of course! Did you see my dunk was at the other room?\n",
    "#   Peter: Let's see. He died in!\n",
    "#   Stewie: I swear to God, he didn't know he's dead!\n",
    "#   Peter: Great, huh? Can I see your dad stinks? He found the dad gets\n",
    "#   firest, Lois, but we gotta be him for a family after lunch with the\n",
    "#   filler. Here comes the White Lagmie Prince things]\n",
    "# [closing theme music]\n",
    "\n",
    "# Title: Da Me Life Over\n",
    "\n",
    "# Theme Song\n",
    "#   Lois: Oh, I'm sorry. They have to face a little further\n",
    "#   reason I call a responsibility to jai\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # import torch\n",
    "# # # from FlashMetal import FlashAttentionMPS\n",
    "\n",
    "# q = torch.randn(batch_size, n_heads, block_size, 96, requires_grad=True, device=\"mps\")\n",
    "# k = torch.randn(batch_size, n_heads, block_size, 96, requires_grad=True, device=\"mps\")\n",
    "# v = torch.randn(batch_size, n_heads, block_size, 96, requires_grad=True, device=\"mps\")\n",
    "\n",
    "# out = FlashAttentionAutograd.apply(q,k,v)\n",
    "# out.retain_grad()\n",
    "# # loss\n",
    "# l1 = torch.mean(out)\n",
    "# l1.backward()\n",
    "\n",
    "\n",
    "# dO_test = (out.grad)\n",
    "\n",
    "# s = q @ k.transpose(-1,-2)\n",
    "# s /= np.sqrt(96)\n",
    "\n",
    "# mask = torch.tril(torch.ones_like(s)).to(\"mps\")\n",
    "# s_masked = torch.where(mask == 1, s, torch.tensor(float('-inf')).to(\"mps\"))\n",
    "\n",
    "# P = F.softmax(s_masked, -1)\n",
    "\n",
    "# o_test = (torch.matmul(P, v))\n",
    "\n",
    "# #dO = torch.randn_like(q, device='mps')\n",
    "# o1 = (s_masked @ v)\n",
    "# dP = torch.matmul(dO_test, v.transpose(-1, -2))\n",
    "# dS = P * (dP - torch.sum(dP * P, dim=-1, keepdim=True))\n",
    "\n",
    "# dQ_test = torch.matmul(dS, k)\n",
    "# print(dQ_test)\n",
    "# print(q.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(gpt.embedding_table(torch.zeros(2, dtype=torch.long, device=\"mps\")))\n",
    "\n",
    "# # print(gpt.blocks[0].mhatt.batch_qkv_matrices.weight)\n",
    "\n",
    "# q = torch.randn(1, 1, block_size, 96, requires_grad=True, device=\"mps\")\n",
    "# k = torch.randn(1, 1, block_size, 96, requires_grad=True, device=\"mps\")\n",
    "# v = torch.randn(1, 1, block_size, 96, requires_grad=True, device=\"mps\")\n",
    "\n",
    "# # # # # print(v.is_contiguous())\n",
    "\n",
    "\n",
    "\n",
    "# # # for i in range(1):\n",
    "# o = FlashAttentionAutograd.apply(q,k,v)\n",
    "\n",
    "# # attention\n",
    "# s = q @ k.transpose(-1,-2)\n",
    "# s /= np.sqrt(96)\n",
    "# # apply mask\n",
    "# mask = torch.tril(torch.ones_like(s)).to(\"mps\")\n",
    "# s_masked = torch.where(mask == 1, s, torch.tensor(float('-inf')).to(\"mps\"))\n",
    "\n",
    "# max_vals = torch.max(s, dim=-1)[0]\n",
    "\n",
    "# # exp_att = torch.exp(s_masked - max_vals)\n",
    "# # sum_exp_att = torch.sum(exp_att, dim=-1)\n",
    "# # print(max_vals)\n",
    "\n",
    "# P = F.softmax(s_masked, -1)\n",
    "# # output\n",
    "# o2 = (torch.matmul(P, v))\n",
    "\n",
    "\n",
    "# # print(sum_exp_att.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2 = torch.mean(o2)\n",
    "# l2.backward()\n",
    "# print(q.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
