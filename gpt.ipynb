{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byte Pair Encoder\n",
    "import re\n",
    "from collections import defaultdict\n",
    "training_data = open(\"family-guy.txt\", \"r\")\n",
    "text = training_data.read()\n",
    "charset = \"\".join(sorted(set(text)))\n",
    "\n",
    "tokens = re.split(r'([\\(\\)\\[\\].:,!\\s])', text)\n",
    "tokens = list(filter(lambda x: x != \"\", tokens))\n",
    "\n",
    "tokens = [list(token) for token in tokens]\n",
    "k = 80\n",
    "\n",
    "token_map = defaultdict(int)\n",
    "\n",
    "# prepare initial tokenset of purely characters\n",
    "for ch in text:\n",
    "    token_map[ch] += 1\n",
    "\n",
    "\n",
    "for _ in range(k):\n",
    "    updated_lst = []\n",
    "    bpe_table = defaultdict(int)\n",
    "    # find the highest frequency byte\n",
    "    for token in tokens:\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        for i in range(len(token) - 1):\n",
    "            bpe_table[\"\".join(token[i:i+2])] += 1\n",
    "\n",
    "    # get maximum frequency byte\n",
    "    max_bp = max(bpe_table, key=bpe_table.get)\n",
    "\n",
    "    for token in tokens:\n",
    "        temp_token = []\n",
    "        if len(token) == 1:\n",
    "            updated_lst.append(token)\n",
    "            continue\n",
    "\n",
    "        cont_flag = 0\n",
    "\n",
    "        # go over each character in token\n",
    "        for i in range(len(token)):\n",
    "            # avoid repeating tokens        \n",
    "            if cont_flag:\n",
    "                cont_flag = 0\n",
    "                continue\n",
    "            if i != len(token) - 1:\n",
    "                pair = token[i:i+2]\n",
    "                # print(pair)\n",
    "                if \"\".join(pair) == max_bp:\n",
    "                    temp_token.append(max_bp)\n",
    "                    token_map[max_bp] += 1\n",
    "                    token_map[pair[0]] -= 1\n",
    "                    token_map[pair[1]] -= 1\n",
    "                    cont_flag = 1\n",
    "                    continue\n",
    "\n",
    "            # else add character\n",
    "            temp_token.append(token[i])\n",
    "            token_map[token[i]] += 1\n",
    "            \n",
    "        updated_lst.append(temp_token)\n",
    "\n",
    "    # set tokens to updated_lst of merged token\n",
    "    tokens = updated_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_set = sorted(list(set([token_key for token_key in token_map if token_map[token_key] > -1])))\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "bpe_text_in = list(chain.from_iterable(updated_lst))\n",
    "\n",
    "# # training_data = open(\"family-guy.txt\", \"r\")\n",
    "# # training_data = training_data.read()\n",
    "# # charset = \"\".join(sorted(set(training_data)))\n",
    "\n",
    "# def encoder(text_in):\n",
    "#     pass\n",
    "            \n",
    "\n",
    "encoder = lambda text_in: [token_set.index(s) for s in text_in]\n",
    "decoder = lambda indices: [token_set[index] for index in indices]\n",
    "device = \"mps\"\n",
    "\n",
    "# token_set\n",
    "# print(token_map)\n",
    "# token_set[-40:]\n",
    "# encoder(bpe_text_in[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m block_size \u001b[39m=\u001b[39m \u001b[39m380\u001b[39m\n\u001b[1;32m      7\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m----> 8\u001b[0m vocab_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(token_set)\n\u001b[1;32m      9\u001b[0m n_embed \u001b[39m=\u001b[39m \u001b[39m80\u001b[39m\u001b[39m*\u001b[39m\u001b[39m6\u001b[39m\n\u001b[1;32m     10\u001b[0m dropout \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'token_set' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "# context window of 512\n",
    "block_size = 380\n",
    "batch_size = 64\n",
    "vocab_size = len(token_set)\n",
    "n_embed = 80*6\n",
    "dropout = 0.2\n",
    "n_heads = 6\n",
    "num_blocks = 6\n",
    "n_steps = 5000\n",
    "\n",
    "\n",
    "data = torch.tensor(encoder(bpe_text_in), dtype=torch.long, device=device)\n",
    "n = int(0.9*len(data))\n",
    "training_set = data[:n]\n",
    "validation_set = data[n:]\n",
    "\n",
    "\n",
    "def generate_batch():\n",
    "        randlst = torch.randint(len(training_set) - block_size, (batch_size,))#.to(device=\"mps\")\n",
    "        batch = torch.stack([training_set[i:i+block_size] for i in randlst])#.to(device=\"mps\")\n",
    "        targets = torch.stack([training_set[i+1: i+block_size+1] for i in randlst])#.to(device=\"mps\")\n",
    "        return batch,targets\n",
    "\n",
    "# x,y = generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # print(head_size)\n",
    "        self.head_size = head_size\n",
    "        self.batch_qkv_matrices = nn.Linear(n_embed, head_size * n_heads * 3, bias=False) \n",
    "        # self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        # self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        # self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones((block_size, block_size))))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        # initially tensor of B,T,C\n",
    "        q,k,v = self.batch_qkv_matrices(x).split(self.head_size * n_heads, dim=-1) # Now Q,K,V of dim B, T, head size * n_heads\n",
    "        \n",
    "        B,T,C = x.shape\n",
    "        # print(q.shape)\n",
    "        # assert C % n_heads == 0\n",
    "        # print(q.shape)\n",
    "        q = q.view(B, T, n_heads, self.head_size).transpose(1,2) # Now of shape B, n_heads, T, head_size for BMM\n",
    "        k = k.view(B, T, n_heads, self.head_size).transpose(1,2)\n",
    "        v = v.view(B, T, n_heads, self.head_size).transpose(1,2)\n",
    "        # transpose because that's how matmul works\n",
    "        weight_mat = q @ k.transpose(-2, -1)\n",
    "        weight_mat = weight_mat * (self.head_size ** -0.5) # \n",
    "        # print(weight_mat.shape)\n",
    "        weight_mat = weight_mat.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        weight_mat = F.softmax(weight_mat, dim=-1)\n",
    "        # # regularisation prevent overfitting\n",
    "        weight_mat = self.dropout(weight_mat)\n",
    "\n",
    "        # print(v.shape, weight_mat.shape)\n",
    "\n",
    "        res = weight_mat @ v\n",
    "        res = res.transpose(1,2) # B, n_heads, T, C --> B, T, n_heads, C\n",
    "        res = res.contiguous().view(B, T, C)\n",
    "        # print(res.shape)\n",
    "        return res\n",
    "\n",
    "# head = Head(64)\n",
    "# x = torch.randn((4, 5, n_embed))\n",
    "# head(x)\n",
    "# \n",
    "# \n",
    "class MHAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.att_heads = Head(head_size=head_size) #nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        res = self.att_heads(x)\n",
    "        # res = torch.cat([att_head(x) for att_head in self.att_heads], dim=-1)\n",
    "        res = self.dropout(self.projection(res))\n",
    "        return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, n_embed) -> None:\n",
    "        super().__init__()\n",
    "        scale_factor = 4\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed * scale_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed * scale_factor, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.ff = Feedforward(n_embed)\n",
    "        self.mhatt = MHAttention(n_heads, (n_embed // n_heads))\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embed) \n",
    "        self.layer_norm2 = nn.LayerNorm(n_embed)\n",
    "    def forward(self, x):\n",
    "        x = x + self.mhatt(self.layer_norm1(x))\n",
    "        x = x + self.ff(self.layer_norm2(x))\n",
    "        return x \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramEmbeddingModel(\n",
       "  (embedding_table): Embedding(188, 480)\n",
       "  (pos_embedding_table): Embedding(380, 480)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (ff): Feedforward(\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1920, out_features=480, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mhatt): MHAttention(\n",
       "        (att_heads): Head(\n",
       "          (batch_qkv_matrices): Linear(in_features=480, out_features=1440, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (projection): Linear(in_features=480, out_features=480, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (ff): Feedforward(\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1920, out_features=480, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mhatt): MHAttention(\n",
       "        (att_heads): Head(\n",
       "          (batch_qkv_matrices): Linear(in_features=480, out_features=1440, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (projection): Linear(in_features=480, out_features=480, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (ff): Feedforward(\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1920, out_features=480, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mhatt): MHAttention(\n",
       "        (att_heads): Head(\n",
       "          (batch_qkv_matrices): Linear(in_features=480, out_features=1440, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (projection): Linear(in_features=480, out_features=480, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (ff): Feedforward(\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1920, out_features=480, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mhatt): MHAttention(\n",
       "        (att_heads): Head(\n",
       "          (batch_qkv_matrices): Linear(in_features=480, out_features=1440, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (projection): Linear(in_features=480, out_features=480, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (ff): Feedforward(\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1920, out_features=480, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mhatt): MHAttention(\n",
       "        (att_heads): Head(\n",
       "          (batch_qkv_matrices): Linear(in_features=480, out_features=1440, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (projection): Linear(in_features=480, out_features=480, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (ff): Feedforward(\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1920, out_features=480, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mhatt): MHAttention(\n",
       "        (att_heads): Head(\n",
       "          (batch_qkv_matrices): Linear(in_features=480, out_features=1440, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (projection): Linear(in_features=480, out_features=480, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "  (lin1): Linear(in_features=480, out_features=188, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "       \n",
    "        # self.att = Head(n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_heads) for _ in range(num_blocks)])\n",
    "\n",
    "        self.layernorm =  nn.LayerNorm(n_embed)\n",
    "\n",
    "        self.lin1 = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "        # self.att = MHAttention(n_heads, n_embed // n_heads)\n",
    "        # self.ff = Feedforward(n_embed)\n",
    "        \n",
    "    def forward(self, data, target=None):\n",
    "        token_layer = self.embedding_table(data)\n",
    "        # print(token_layer.shape)\n",
    "        B,T = data.shape\n",
    "        pos_embed = self.pos_embedding_table(torch.arange(T).to(device))\n",
    "        total = token_layer + pos_embed\n",
    "        total = self.layernorm(self.blocks(total))\n",
    "        logits = self.lin1(total)\n",
    "          \n",
    "        if target != None:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C) \n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "            return logits, loss\n",
    "        return logits, None\n",
    "    \n",
    "\n",
    "    def predict(self, data, num_tokens=100):\n",
    "        curr = data\n",
    "        for _ in range(num_tokens):\n",
    "            data_mod = curr[:, -block_size:]\n",
    "            # print(data.shape)\n",
    "            logits,lossNone = self(data_mod)\n",
    "\n",
    "            logits = logits[:,-1,:]\n",
    "            prob_dist = F.softmax(logits, dim=-1)\n",
    "            sample = torch.multinomial(prob_dist, num_samples=1)\n",
    "            curr = torch.cat((curr, sample), dim=1)\n",
    "\n",
    "        return curr\n",
    "\n",
    "gpt = GPT().to(device)\n",
    "gpt.train()\n",
    "# logits, loss = bgm(x,y)\n",
    "# bgm()\n",
    "# print(\"\".join(decoder(bgm.predict(data=torch.zeros(1,1, dtype=torch.long))[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training loop\n",
    "\n",
    "# optimiser = torch.optim.AdamW(bgm.parameters(), lr=3e-4)\n",
    "# for steps in range(n_steps):\n",
    "#     batch,target = generate_batch()\n",
    "#     logits, loss = bgm(batch,target)\n",
    "#     optimiser.zero_grad(set_to_none=True)\n",
    "#     loss.backward()\n",
    "    \n",
    "#     optimiser.step() \n",
    "#     if steps % 50 == 0:\n",
    "#         print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter: Come on in,  fella. Great news Lowe husband? Come\n",
      "  on!\n",
      "  Stewie: Well, here we are. Chris, your father and show them who are\n",
      "  still family.\n",
      "  Back to Peter & Lois in the outside. And Meg is beginning to Brian\n",
      "  and Stewie are still with a child big stretch\n",
      "  beangst the night birthday.\n",
      "  [Alarming instrumental music]\n",
      "  Lois: Oh, Brian! Okay, I don't know any of this, did you take this army\n",
      "  jouster?\n",
      "  Brian: Thanks anyway. Devon this far before I farted, I have Calpa\n",
      "  Farpet Farm.\n",
      "  Chris: You don't find your insideeeeeeeeeeeeeeeeeeeeeee\n"
     ]
    }
   ],
   "source": [
    "test = torch.tensor(encoder([\"Peter\", \":\", \" \"]), dtype=torch.long, device=device)\n",
    "# print(test)\n",
    "# # # # # print(test.shape)\n",
    "K = 1\n",
    "test = test.unsqueeze(0).repeat(K, 1)\n",
    "new_model = gpt().to(device)\n",
    "new_model.load_state_dict(torch.load(\"family-guy-lm-BPE-C380-E80\"))\n",
    "new_model.eval()\n",
    "# # # # print(test.shape)\n",
    "# # # # bgm.eval()\n",
    "# # # family_guy_file = open(\"family-guy-text-400.txt\", \"w+\")\n",
    "\n",
    " \n",
    "# # # # bgm.eval()\n",
    "with torch.no_grad():\n",
    "# #     pass\n",
    "# #     # print(\"\".join(decoder(new_model.predict(torch.zeros(1,1, dtype=torch.long).to(device), num_tokens=20)[0].tolist())))\n",
    "    out = new_model.predict(test, num_tokens=400)\n",
    "#     # print(out)\n",
    "    print(\"\".join(decoder(out[0].tolist())))\n",
    "# # #     for i in range(K): \n",
    "# # #         family_guy_file.write(\"\".join(decoder(out[i].tolist())))\n",
    "# # #         family_guy_file.write(\"\\n\")\n",
    "\n",
    "# # # family_guy_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B,T,C = 4,8,2\n",
    "# rand_tensor = torch.randn(size=(B,T,C))\n",
    "# avg_tensor = torch.zeros(size=(B,T,C))\n",
    "# for b in range((B)):\n",
    "#     for t in range(T):\n",
    "#         window = rand_tensor[b, :t+1]\n",
    "#         avg_tensor[b,t] = torch.mean(window,0)\n",
    "# avg_tensor[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_matrix = torch.tril(torch.ones((T,T)))\n",
    "# affinity = torch.zeros((T,T))\n",
    "# affinity = affinity.masked_fill(filter_matrix == 0, float(\"-inf\"))\n",
    "# affinity = F.softmax(affinity, dim=1)\n",
    "# affinity @ rand_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(bgm.state_dict(), \"family-guy-lm-BPE-C380-E80\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16981628"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in gpt.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
