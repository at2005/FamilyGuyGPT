{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Byte Pair Encoder\n",
    "# import re\n",
    "# from collections import defaultdict\n",
    "# training_data = open(\"family-guy.txt\", \"r\")\n",
    "# text = training_data.read()\n",
    "# charset = \"\".join(sorted(set(text)))\n",
    "\n",
    "# tokens = re.split(r'([\\(\\)\\[\\].:,!\\s])', text)\n",
    "# tokens = list(filter(lambda x: x != \"\", tokens))\n",
    "\n",
    "# tokens = [list(token) for token in tokens]\n",
    "# k = 80\n",
    "\n",
    "# token_map = defaultdict(int)\n",
    "\n",
    "# # prepare initial tokenset of purely characters\n",
    "# for ch in text:\n",
    "#     token_map[ch] += 1\n",
    "\n",
    "\n",
    "# for _ in range(k):\n",
    "#     updated_lst = []\n",
    "#     bpe_table = defaultdict(int)\n",
    "#     # find the highest frequency byte\n",
    "#     for token in tokens:\n",
    "#         if len(token) == 1:\n",
    "#             continue\n",
    "#         for i in range(len(token) - 1):\n",
    "#             bpe_table[\"\".join(token[i:i+2])] += 1\n",
    "\n",
    "#     # get maximum frequency byte\n",
    "#     max_bp = max(bpe_table, key=bpe_table.get)\n",
    "\n",
    "#     for token in tokens:\n",
    "#         temp_token = []\n",
    "#         if len(token) == 1:\n",
    "#             updated_lst.append(token)\n",
    "#             continue\n",
    "\n",
    "#         cont_flag = 0\n",
    "\n",
    "#         # go over each character in token\n",
    "#         for i in range(len(token)):\n",
    "#             # avoid repeating tokens        \n",
    "#             if cont_flag:\n",
    "#                 cont_flag = 0\n",
    "#                 continue\n",
    "#             if i != len(token) - 1:\n",
    "#                 pair = token[i:i+2]\n",
    "#                 # print(pair)\n",
    "#                 if \"\".join(pair) == max_bp:\n",
    "#                     temp_token.append(max_bp)\n",
    "#                     token_map[max_bp] += 1\n",
    "#                     token_map[pair[0]] -= 1\n",
    "#                     token_map[pair[1]] -= 1\n",
    "#                     cont_flag = 1\n",
    "#                     continue\n",
    "\n",
    "#             # else add character\n",
    "#             temp_token.append(token[i])\n",
    "#             token_map[token[i]] += 1\n",
    "            \n",
    "#         updated_lst.append(temp_token)\n",
    "\n",
    "#     # set tokens to updated_lst of merged token\n",
    "#     tokens = updated_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", \"'s\", \"'t\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'He', 'I', \"I'\", \"I'm\", 'J', 'K', 'L', 'Lo', 'Lois', 'M', 'N', 'O', 'Oh', 'P', 'Peter', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'We', 'X', 'Y', 'You', 'Z', '[', '\\\\', ']', '`', 'a', 'ac', 'ad', 'al', 'all', 'an', 'and', 'ar', 'as', 'at', 'ay', 'b', 'be', 'c', 'ch', 'd', 'do', 'e', 'ed', 'en', 'er', 'ere', 'es', 'et', 'eter', 'f', 'for', 'g', 'gh', 'ght', 'h', 'ha', 'hat', 'he', 'i', 'ic', 'id', 'in', 'ing', 'is', 'it', 'j', 'k', 'ke', 'l', 'ld', 'le', 'li', 'll', 'm', 'me', 'my', 'n', 'nd', 'now', 'o', 'of', 'om', 'ome', 'on', 'one', 'oo', 'or', 'ot', 'ou', 'out', 'ow', 'p', 'q', 'r', 're', 'ri', 's', 'se', 'st', 't', 'te', 'th', 'that', 'the', 'this', 'to', 'u', 'ust', 'ut', 'v', 've', 'ver', 'w', 'we', 'wi', 'x', 'y', 'you', 'z', '{', '}', '\\xa0', '©', 'ª', '°', 'á', 'â', 'é', 'í', 'ñ', 'ó', '—', '‘', '’', '“', '”', '…', '™', '♪']\n"
     ]
    }
   ],
   "source": [
    "# token_set = sorted(list(set([token_key for token_key in token_map if token_map[token_key] > -1])))\n",
    "# print(token_set)\n",
    "\n",
    "# save token_set to file\n",
    "# with open(\"token_set.txt\", \"w\") as f:\n",
    "    # f.write(\"±\".join(token_set))\n",
    "\n",
    "# load token_set from file\n",
    "with open(\"token_set.txt\", \"r\") as f:\n",
    "    token_set = f.read().split(\"±\")\n",
    "\n",
    "print(token_set)\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "# bpe_text_in = list(chain.from_iterable(updated_lst))\n",
    "\n",
    "# # training_data = open(\"family-guy.txt\", \"r\")\n",
    "# # training_data = training_data.read()\n",
    "# # charset = \"\".join(sorted(set(training_data)))\n",
    "\n",
    "# def encoder(text_in):\n",
    "#     pass\n",
    "            \n",
    "\n",
    "encoder = lambda text_in: [token_set.index(s) for s in text_in]\n",
    "decoder = lambda indices: [token_set[index] for index in indices]\n",
    "device = \"mps\"\n",
    "\n",
    "# token_set\n",
    "# print(token_map)\n",
    "# token_set[-40:]\n",
    "# encoder(bpe_text_in[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "# context window of 512\n",
    "block_size = 380\n",
    "batch_size = 64\n",
    "vocab_size = len(token_set)\n",
    "n_embed = 80*6\n",
    "dropout = 0.2\n",
    "n_heads = 6\n",
    "num_blocks = 6\n",
    "n_steps = 5000\n",
    "\n",
    "\n",
    "# data = torch.tensor(encoder(bpe_text_in), dtype=torch.long, device=device)\n",
    "# save tensor to file \n",
    "# torch.save(data, 'data.pt')\n",
    "# load tensor from file\n",
    "data = torch.load('data.pt')\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "training_set = data[:n]\n",
    "validation_set = data[n:]\n",
    "\n",
    "\n",
    "def generate_batch():\n",
    "        randlst = torch.randint(len(training_set) - block_size, (batch_size,))#.to(device=\"mps\")\n",
    "        batch = torch.stack([training_set[i:i+block_size] for i in randlst])#.to(device=\"mps\")\n",
    "        targets = torch.stack([training_set[i+1: i+block_size+1] for i in randlst])#.to(device=\"mps\")\n",
    "        return batch,targets\n",
    "\n",
    "# x,y = generate_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # print(head_size)\n",
    "        self.head_size = head_size\n",
    "        self.batch_qkv_matrices = nn.Linear(n_embed, head_size * n_heads * 3, bias=False) \n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones((block_size, block_size))))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.kv_cache = None\n",
    "    \n",
    "    # def reset_cache(self):\n",
    "    #     self.kv_cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # initially tensor of B,T,C\n",
    "   \n",
    "        q,k,v = self.batch_qkv_matrices(x).split(self.head_size * n_heads, dim=-1) # Now Q,K,V of dim B, T, head size * n_heads\n",
    "       \n",
    "        B,T,C = x.shape\n",
    "        # # assert C % n_heads == 0\n",
    "        T_Q = T\n",
    "        T_KV = T\n",
    "        # if model in inference (eval) mode then use cache\n",
    "        if not self.training:\n",
    "            if self.kv_cache is not None: \n",
    "                # concat new kv onto old kv cache -> update cache\n",
    "                k_prev,v_prev = self.kv_cache\n",
    "                # reset cache if block size reached\n",
    "                if self.kv_cache[0].shape[-2] < block_size:\n",
    "                    k = torch.cat((k_prev, k), dim=1)\n",
    "                    v = torch.cat((v_prev, v), dim=1)\n",
    "                    # trim key-value cache to block size\n",
    "                    # k = k[:, -block_size:, :]\n",
    "                    # v = v[:, -block_size:, :]\n",
    "\n",
    "                # set T_Q to 1 as we have 1-d query\n",
    "                T_Q = 1\n",
    "            \n",
    "            T_KV = k.shape[-2]            \n",
    "            self.kv_cache = (k,v)\n",
    "\n",
    " \n",
    "        k = k.view(B, T_KV, n_heads, self.head_size).transpose(1,2)\n",
    "        # # print(f\"K: {k.shape}\")\n",
    "        q = q.view(B, T_Q, n_heads, self.head_size).transpose(1,2) # Now of shape B, n_heads, T, head_size for BMM\n",
    "        v = v.view(B, T_KV, n_heads, self.head_size).transpose(1,2)\n",
    "   \n",
    "        weight_mat = q @ k.transpose(-2, -1)\n",
    "\n",
    "        weight_mat = weight_mat * (self.head_size ** -0.5) #\n",
    "\n",
    "        if self.training:\n",
    "            weight_mat = weight_mat.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "\n",
    "        weight_mat = F.softmax(weight_mat, dim=-1)\n",
    "        # # regularisation prevent overfitting\n",
    "        weight_mat = self.dropout(weight_mat)\n",
    "        # Multiply with values\n",
    "        res = weight_mat @ v\n",
    "        res = res.transpose(1,2) # B, n_heads, T, C --> B, T, n_heads, C   \n",
    "        res = res.contiguous().view(B, T, C)\n",
    "        return res\n",
    "    \n",
    "    # COMMENTED OUT FOR TESTING\n",
    "\n",
    "\n",
    "class MHAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.att_heads = Head(head_size=head_size) #nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        res = self.att_heads(x)\n",
    "        # res = torch.cat([att_head(x) for att_head in self.att_heads], dim=-1)\n",
    "        res = self.dropout(self.projection(res))\n",
    "        return res \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, n_embed) -> None:\n",
    "        super().__init__()\n",
    "        scale_factor = 4\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed * scale_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed * scale_factor, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.ff = Feedforward(n_embed)\n",
    "        self.mhatt = MHAttention(n_heads, (n_embed // n_heads))\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embed) \n",
    "        self.layer_norm2 = nn.LayerNorm(n_embed)\n",
    "    def forward(self, x):\n",
    "        x = x + self.mhatt(self.layer_norm1(x))\n",
    "        x = x + self.ff(self.layer_norm2(x))\n",
    "        return x \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "       \n",
    "        # self.att = Head(n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_heads) for _ in range(num_blocks)])\n",
    "\n",
    "        self.layernorm =  nn.LayerNorm(n_embed)\n",
    "\n",
    "        self.lin1 = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "        # self.att = MHAttention(n_heads, n_embed // n_heads)\n",
    "        # self.ff = Feedforward(n_embed)\n",
    "        \n",
    "    def forward(self, data, target=None, time_inf=None):\n",
    "        # print(data.shape)\n",
    "        token_layer = self.embedding_table(data)\n",
    "        # print(token_layer.shape)\n",
    "        B,T = data.shape\n",
    "        \n",
    "        if time_inf != None:\n",
    "            # pos_embed arguement must be tensor, must be single value of time_inf \n",
    "            pos_embed = self.pos_embedding_table(torch.ones(1, dtype=torch.long).to(device) * time_inf)\n",
    "        else:\n",
    "            pos_embed = self.pos_embedding_table(torch.arange(T).to(device))\n",
    "        \n",
    "        total = token_layer + pos_embed\n",
    "        # print(f\"INPUT {total.shape}\")\n",
    "\n",
    "        total = self.layernorm(self.blocks(total))\n",
    "        logits = self.lin1(total)\n",
    "\n",
    "        if target != None:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C) \n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "            return logits, loss\n",
    "        return logits, None\n",
    "        \n",
    "\n",
    "    def predict(self, data, temperature=1, num_tokens=350):\n",
    "        prev_token = data\n",
    "\n",
    "        for i in range(num_tokens):\n",
    "            \n",
    "            t_curr = (i+data.shape[-1]) % block_size\n",
    "\n",
    "            logits,lossNone = self(prev_token, time_inf=t_curr)\n",
    "\n",
    "            logits = logits[:,-1,:]\n",
    "            logits = logits / temperature\n",
    "            prob_dist = F.softmax(logits, dim=-1)\n",
    "            prev_token = torch.multinomial(prob_dist, num_samples=1)\n",
    "            print(decoder(prev_token[0].tolist())[0],end=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training loop\n",
    "\n",
    "# gpt = GPT().to(device)       \n",
    "# gpt.train()\n",
    "# optimiser = torch.optim.AdamW(gpt.parameters(), lr=3e-4)\n",
    "# for steps in range(n_steps):\n",
    "#     batch,target = generate_batch()\n",
    "#     logits, loss = gpt(batch,target)\n",
    "#     optimiser.zero_grad(set_to_none=True)\n",
    "#     loss.backward()\n",
    "    \n",
    "#     optimiser.step() \n",
    "#     if steps % 50 == 0:\n",
    "#         print(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text...\n",
      "Title: the Griffin has 17. That guy show on the $20\n",
      "  batter of 20. (Flashback to the T Curbank Points to Lois\n",
      "  Griffin's house.>\n",
      "  Peter: \"Ah, well there it is sexy.\"\n",
      "  Lois: \"Well, well were different Adult Cameraman, honey, I'd love you, but\n",
      "  I was wonderful for Monday And Daniel Cameraman, I don't think I\n",
      "  caught anything, but I didn't appreciate you from me. My mother, Peter put\n",
      "  you so funny.\"\n",
      "  Lois: \"So the hell think you could make you Peter put your back!\"\n",
      "  Peter: \"But, I just don't kind of crap!\"\n",
      " Loretta: He's not going to see your walk.\n",
      "  Stewie: You said that on your walk, if you don't have to\n",
      "  do anything, does! I'm going to swore you throw about it.\n",
      "  Lois: Peter, you better steal with the john about the other damage bar. Want\n",
      "  some bar? 'Cause I have the suffer on?\n",
      "  Lois: It's not good at me, but you're not a very idea for a provide back\n",
      "  chews.\n",
      "  Peter: Really? I don't think it is.\n",
      "  Lois: Honey, I'm hurting you.\n",
      "  Peter: Honey, you help me! You know what, Brian, you're the one who can get\n",
      "  it? What are y'all set? These a?\n",
      "  Peter: Oh, you're just trying to think of this, Brian?\n",
      "  Brian: No. It's already hot groans and have my penis driver as we knew we\n",
      "  met after he's been lost. I just needed. Hey there, Lois, Brian, if you keep\n",
      "  up, you write on my kids because it's been a little bit in squor.\n",
      "  Lois: Oh, no. Peter. You're on the only one nut.\n",
      "  Peter: Yeah! Are you okay?\n",
      "  Brian: Yeah, I'll do it you down for the news.\n",
      "  Peter: Aw, crap, Lois. I was just, uh, would you really miss you, too?\n",
      "  Lois: I just got that lame\n",
      "  society.\n",
      "  Peter: Aw, Hear quickling out for the name of a whole landed\n",
      "  of the mind-control that boy...\n",
      "  Stewie: ...for dinner? He spits there. Hey, uh, I flung, Meg?\n",
      "  Stewie: ...uh, hi, how in your mind, my phone can only get Stewie's gonna\n",
      "  be Test Employee and I want to share our phone. Just watch over\n",
      "  here and dance around and watch the biddas, my life in the kitchen\n",
      "  products out watching holds up with her\n",
      "  panties and he hollow him real father. He's a vacation.\n",
      "  Just like Stewie runs out the doorway.\n",
      "  Old Yelly Rickpit coufare.\n",
      "  Peter, they've got a few days of pimple juice to run around with them\n",
      "  for you both large.\n",
      "  Where were you?\n",
      "  Well, I love some date\n",
      "  with Blues,\n",
      "  and I am always there where my mind.\n",
      "  And I have some ways to smoke back at them with the sofap change.\n",
      "  And I have no idea.\n",
      "  What?!\n",
      "  Look at me, sir.\n",
      "  Now, the end of your mind.\n",
      "  Yes.\n",
      "  Oh! Look at me!\n",
      "  What?!\n",
      "  You can't get to you.\n",
      "  What do you mean, man!\n",
      "  What the hell?!\n",
      "  I haven't done last time.\n",
      "  Well, with you...\n",
      "  shut up.\n",
      "  Oh Brian: Ooh, that sucks! That's not me. It's not hot.\n",
      "  That's not funny. I'm dood. That's apploe.\n",
      "  Dr. Kaplan: Okay, here's what it is."
     ]
    }
   ],
   "source": [
    "prompt = \"Title: \"\n",
    "gpt = GPT().to(device)       \n",
    "test = torch.tensor(encoder([\"T\", \"it\", \"le\", \":\", \" \"]), dtype=torch.long, device=device)\n",
    "# print(test)\n",
    "# # # # # print(test.shape)\n",
    "K = 1\n",
    "test = test.unsqueeze(0).repeat(K, 1)\n",
    "new_model = gpt.to(device)\n",
    "new_model.load_state_dict(torch.load(\"family-guy-lm-BPE-C380-E80\"))\n",
    "new_model.eval()\n",
    "\n",
    "# # # # # gpt.eval()\n",
    "with torch.no_grad():\n",
    "    print(\"Generating text...\")\n",
    "    print(prompt, end=\"\")\n",
    "    out = new_model.predict(test, num_tokens=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 16981628\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters: {sum(p.numel() for p in gpt.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peter: AH! Agh! We've handicapped that crowd on\n",
    "#   my power over and be a Chootian Banks. Or is cookie\n",
    "#   deer you've gotten telling me your daughter's kindly but\n",
    "#   I see my life will turn into a box criminal white, \"Dear I will\n",
    "#   have to leave a Salt Banks.\" And sometimes I can see my family girl\n",
    "#   where it to be Hic-a-doo-la guy was on his crowd and she was on screen.\n",
    "#   Peter: Wow! Wait, wait, wait. You can't leave. You end up on it.\n",
    "#   Caddie: You're a nut! Hey, how could you? Oh, Stewie, what about\n",
    "#   yoursel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt.eval()\n",
    "# gpt(torch.tensor([[0]]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
