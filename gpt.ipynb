{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Byte Pair Encoder\n",
    "# import re\n",
    "# from collections import defaultdict\n",
    "# training_data = open(\"family-guy.txt\", \"r\")\n",
    "# text = training_data.read()\n",
    "# charset = \"\".join(sorted(set(text)))\n",
    "\n",
    "# tokens = re.split(r'([\\(\\)\\[\\].:,!\\s])', text)\n",
    "# tokens = list(filter(lambda x: x != \"\", tokens))\n",
    "\n",
    "# tokens = [list(token) for token in tokens]\n",
    "# k = 80\n",
    "\n",
    "# token_map = defaultdict(int)\n",
    "\n",
    "# # prepare initial tokenset of purely characters\n",
    "# for ch in text:\n",
    "#     token_map[ch] += 1\n",
    "\n",
    "\n",
    "# for _ in range(k):\n",
    "#     updated_lst = []\n",
    "#     bpe_table = defaultdict(int)\n",
    "#     # find the highest frequency byte\n",
    "#     for token in tokens:\n",
    "#         if len(token) == 1:\n",
    "#             continue\n",
    "#         for i in range(len(token) - 1):\n",
    "#             bpe_table[\"\".join(token[i:i+2])] += 1\n",
    "\n",
    "#     # get maximum frequency byte\n",
    "#     max_bp = max(bpe_table, key=bpe_table.get)\n",
    "\n",
    "#     for token in tokens:\n",
    "#         temp_token = []\n",
    "#         if len(token) == 1:\n",
    "#             updated_lst.append(token)\n",
    "#             continue\n",
    "\n",
    "#         cont_flag = 0\n",
    "\n",
    "#         # go over each character in token\n",
    "#         for i in range(len(token)):\n",
    "#             # avoid repeating tokens        \n",
    "#             if cont_flag:\n",
    "#                 cont_flag = 0\n",
    "#                 continue\n",
    "#             if i != len(token) - 1:\n",
    "#                 pair = token[i:i+2]\n",
    "#                 # print(pair)\n",
    "#                 if \"\".join(pair) == max_bp:\n",
    "#                     temp_token.append(max_bp)\n",
    "#                     token_map[max_bp] += 1\n",
    "#                     token_map[pair[0]] -= 1\n",
    "#                     token_map[pair[1]] -= 1\n",
    "#                     cont_flag = 1\n",
    "#                     continue\n",
    "\n",
    "#             # else add character\n",
    "#             temp_token.append(token[i])\n",
    "#             token_map[token[i]] += 1\n",
    "            \n",
    "#         updated_lst.append(temp_token)\n",
    "\n",
    "#     # set tokens to updated_lst of merged token\n",
    "#     tokens = updated_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set = sorted(list(set([token_key for token_key in token_map if token_map[token_key] > -1])))\n",
    "# print(token_set)\n",
    "\n",
    "# save token_set to file\n",
    "# with open(\"token_set.txt\", \"w\") as f:\n",
    "    # f.write(\"±\".join(token_set))\n",
    "\n",
    "# load token_set from file\n",
    "with open(\"token_set.txt\", \"r\") as f:\n",
    "    token_set = f.read().split(\"±\")\n",
    "\n",
    "print(token_set)\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "# bpe_text_in = list(chain.from_iterable(updated_lst))\n",
    "\n",
    "# # training_data = open(\"family-guy.txt\", \"r\")\n",
    "# # training_data = training_data.read()\n",
    "# # charset = \"\".join(sorted(set(training_data)))\n",
    "\n",
    "# def encoder(text_in):\n",
    "#     pass\n",
    "            \n",
    "\n",
    "encoder = lambda text_in: [token_set.index(s) for s in text_in]\n",
    "decoder = lambda indices: [token_set[index] for index in indices]\n",
    "device = \"mps\"\n",
    "\n",
    "# token_set\n",
    "# print(token_map)\n",
    "# token_set[-40:]\n",
    "# encoder(bpe_text_in[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "# context window of 380 tokens\n",
    "block_size = 380\n",
    "batch_size = 64\n",
    "vocab_size = len(token_set)\n",
    "dropout = 0.2\n",
    "n_heads = 7\n",
    "n_embed = 80*n_heads\n",
    "num_blocks = 6\n",
    "n_steps = 9000\n",
    "\n",
    "\n",
    "# data = torch.tensor(encoder(bpe_text_in), dtype=torch.long, device=device)\n",
    "# save tensor to file \n",
    "# torch.save(data, 'data.pt')\n",
    "# load tensor from file\n",
    "data = torch.load('data.pt')\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "training_set = data[:n]\n",
    "validation_set = data[n:]\n",
    "\n",
    "\n",
    "def generate_batch():\n",
    "        randlst = torch.randint(len(training_set) - block_size, (batch_size,))#.to(device=\"mps\")\n",
    "        batch = torch.stack([training_set[i:i+block_size] for i in randlst])#.to(device=\"mps\")\n",
    "        targets = torch.stack([training_set[i+1: i+block_size+1] for i in randlst])#.to(device=\"mps\")\n",
    "        return batch,targets\n",
    "\n",
    "def generate_batch_validation():\n",
    "        randlst = torch.randint(len(validation_set) - block_size, (batch_size,))#.to(device=\"mps\")\n",
    "        batch = torch.stack([validation_set[i:i+block_size] for i in randlst])#.to(device=\"mps\")\n",
    "        targets = torch.stack([validation_set[i+1: i+block_size+1] for i in randlst])#.to(device=\"mps\")\n",
    "        return batch,targets\n",
    "\n",
    "# x,y = generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # print(head_size)\n",
    "        self.head_size = head_size\n",
    "        self.batch_qkv_matrices = nn.Linear(n_embed, head_size * n_heads * 3, bias=False) \n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones((block_size, block_size))))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.kv_cache = None\n",
    "    \n",
    "    # def reset_cache(self):\n",
    "    #     self.kv_cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # initially tensor of B,T,C\n",
    "   \n",
    "        q,k,v = self.batch_qkv_matrices(x).split(self.head_size * n_heads, dim=-1) # Now Q,K,V of dim B, T, head size * n_heads\n",
    "       \n",
    "        B,T,C = x.shape\n",
    "        # # assert C % n_heads == 0\n",
    "        T_Q = T\n",
    "        T_KV = T\n",
    "        # if model in inference (eval) mode then use cache\n",
    "        if not self.training:\n",
    "            if self.kv_cache is not None: \n",
    "                # concat new kv onto old kv cache -> update cache\n",
    "                k_prev,v_prev = self.kv_cache\n",
    "                # reset cache if block size reached\n",
    "                if self.kv_cache[0].shape[-2] < block_size:\n",
    "                    k = torch.cat((k_prev, k), dim=1)\n",
    "                    v = torch.cat((v_prev, v), dim=1)\n",
    "                    # trim key-value cache to block size\n",
    "                    # k = k[:, -block_size:, :]\n",
    "                    # v = v[:, -block_size:, :]\n",
    "\n",
    "                # set T_Q to 1 as we have 1-d query\n",
    "                T_Q = 1\n",
    "            \n",
    "            T_KV = k.shape[-2]            \n",
    "            self.kv_cache = (k,v)\n",
    "\n",
    " \n",
    "        k = k.view(B, T_KV, n_heads, self.head_size).transpose(1,2)\n",
    "        # # print(f\"K: {k.shape}\")\n",
    "        q = q.view(B, T_Q, n_heads, self.head_size).transpose(1,2) # Now of shape B, n_heads, T, head_size for BMM\n",
    "        v = v.view(B, T_KV, n_heads, self.head_size).transpose(1,2)\n",
    "   \n",
    "        weight_mat = q @ k.transpose(-2, -1)\n",
    "\n",
    "        weight_mat = weight_mat * (self.head_size ** -0.5) #\n",
    "\n",
    "        if self.training:\n",
    "            weight_mat = weight_mat.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "\n",
    "        weight_mat = F.softmax(weight_mat, dim=-1)\n",
    "        # # regularisation prevent overfitting\n",
    "        weight_mat = self.dropout(weight_mat)\n",
    "        # Multiply with values\n",
    "        res = weight_mat @ v\n",
    "        res = res.transpose(1,2) # B, n_heads, T, C --> B, T, n_heads, C   \n",
    "        res = res.contiguous().view(B, T, C)\n",
    "        return res\n",
    "    \n",
    "    # COMMENTED OUT FOR TESTING\n",
    "\n",
    "\n",
    "class MHAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.att_heads = Head(head_size=head_size) #nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        res = self.att_heads(x)\n",
    "        # res = torch.cat([att_head(x) for att_head in self.att_heads], dim=-1)\n",
    "        res = self.dropout(self.projection(res))\n",
    "        return res \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, n_embed) -> None:\n",
    "        super().__init__()\n",
    "        scale_factor = 4\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed * scale_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed * scale_factor, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.ff = Feedforward(n_embed)\n",
    "        self.mhatt = MHAttention(n_heads, (n_embed // n_heads))\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embed) \n",
    "        self.layer_norm2 = nn.LayerNorm(n_embed)\n",
    "    def forward(self, x):\n",
    "        x = x + self.mhatt(self.layer_norm1(x))\n",
    "        x = x + self.ff(self.layer_norm2(x))\n",
    "        return x \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "       \n",
    "        # self.att = Head(n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_heads) for _ in range(num_blocks)])\n",
    "\n",
    "        self.layernorm =  nn.LayerNorm(n_embed)\n",
    "\n",
    "        self.lin1 = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "        # self.att = MHAttention(n_heads, n_embed // n_heads)\n",
    "        # self.ff = Feedforward(n_embed)\n",
    "        \n",
    "    def forward(self, data, target=None, time_inf=None):\n",
    "        # print(data.shape)\n",
    "        token_layer = self.embedding_table(data)\n",
    "        # print(token_layer.shape)\n",
    "        B,T = data.shape\n",
    "        \n",
    "        if time_inf != None:\n",
    "            # pos_embed arguement must be tensor, must be single value of time_inf \n",
    "            pos_embed = self.pos_embedding_table(torch.ones(1, dtype=torch.long).to(device) * time_inf)\n",
    "        else:\n",
    "            pos_embed = self.pos_embedding_table(torch.arange(T).to(device))\n",
    "        \n",
    "        total = token_layer + pos_embed\n",
    "        # print(f\"INPUT {total.shape}\")\n",
    "\n",
    "        total = self.layernorm(self.blocks(total))\n",
    "        logits = self.lin1(total)\n",
    "\n",
    "        if target != None:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C) \n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "            return logits, loss\n",
    "        return logits, None\n",
    "        \n",
    "\n",
    "    def predict(self, data, temperature=1, num_tokens=350):\n",
    "        prev_token = data\n",
    "\n",
    "        for i in range(num_tokens):\n",
    "            \n",
    "            t_curr = (i+data.shape[-1] + 1) % block_size\n",
    "\n",
    "            logits,lossNone = self(prev_token, time_inf=t_curr)\n",
    "\n",
    "            logits = logits[:,-1,:]\n",
    "            logits = logits / temperature\n",
    "            prob_dist = F.softmax(logits, dim=-1)\n",
    "            prev_token = torch.multinomial(prob_dist, num_samples=1)\n",
    "            print(decoder(prev_token[0].tolist())[0],end=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training loop\n",
    "\n",
    "# gpt = GPT().to(device)       \n",
    "# gpt.train()\n",
    "# optimiser = torch.optim.AdamW(gpt.parameters(), lr=3e-4)\n",
    "# for steps in range(n_steps):\n",
    "#     batch,target = generate_batch() \n",
    "#     logits, loss = gpt(batch,target)\n",
    "#     optimiser.zero_grad(set_to_none=True)\n",
    "#     loss.backward() \n",
    "#     optimiser.step() \n",
    "\n",
    "\n",
    "#     if steps % 50 == 0:\n",
    "#         # validation_batch, validation_target = generate_batch_validation()\n",
    "#         # validation_logits, validation_loss = gpt(validation_batch,validation_target)\n",
    "#         print(f\"Training Loss: {loss.item()}\")#. Validation Loss: {validation_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Peter: \"\n",
    "gpt = GPT().to(device)       \n",
    "test = torch.tensor(encoder([\"Peter\", \":\", \" \"]) ,dtype=torch.long, device=device)\n",
    "# print(test)\n",
    "# # # # # print(test.shape)\n",
    "K = 1\n",
    "test = test.unsqueeze(0).repeat(K, 1)\n",
    "new_model = gpt.to(device)\n",
    "new_model.load_state_dict(torch.load(\"family-guy-lm-BPE-C380-E80-N9000-H7.pth\"))\n",
    "new_model.eval()\n",
    "\n",
    "# # # # # gpt.eval()\n",
    "with torch.no_grad():\n",
    "    print(\"Generating text...\")\n",
    "    print(prompt, end=\"\")\n",
    "    out = new_model.predict(test, num_tokens=3200, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters: {sum(p.numel() for p in gpt.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
