{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Byte Pair Encoder\n",
    "# import re\n",
    "# from collections import defaultdict\n",
    "# training_data = open(\"scifi.txt\", \"r\")\n",
    "# text = training_data.read()\n",
    "# charset = \"\".join(sorted(set(text)))\n",
    "\n",
    "# tokens = re.split(r'([\\(\\)\\[\\].:,!\\s])', text)\n",
    "# tokens = list(filter(lambda x: x != \"\", tokens))\n",
    "\n",
    "# tokens = [list(token) for token in tokens]\n",
    "# k = 90\n",
    "\n",
    "# token_map = defaultdict(int)\n",
    "\n",
    "# # prepare initial tokenset of purely characters\n",
    "# for ch in text:\n",
    "#     token_map[ch] += 1\n",
    "\n",
    "\n",
    "# for _ in range(k):\n",
    "#     updated_lst = []\n",
    "#     bpe_table = defaultdict(int)\n",
    "#     # find the highest frequency byte\n",
    "#     for token in tokens:\n",
    "#         if len(token) == 1:\n",
    "#             continue\n",
    "#         for i in range(len(token) - 1):\n",
    "#             bpe_table[\"\".join(token[i:i+2])] += 1\n",
    "\n",
    "#     # get maximum frequency byte\n",
    "#     max_bp = max(bpe_table, key=bpe_table.get)\n",
    "\n",
    "#     for token in tokens:\n",
    "#         temp_token = []\n",
    "#         if len(token) == 1:\n",
    "#             updated_lst.append(token)\n",
    "#             continue\n",
    "\n",
    "#         cont_flag = 0\n",
    "\n",
    "#         # go over each character in token\n",
    "#         for i in range(len(token)):\n",
    "#             # avoid repeating tokens        \n",
    "#             if cont_flag:\n",
    "#                 cont_flag = 0\n",
    "#                 continue\n",
    "#             if i != len(token) - 1:\n",
    "#                 pair = token[i:i+2]\n",
    "#                 # print(pair)\n",
    "#                 if \"\".join(pair) == max_bp:\n",
    "#                     temp_token.append(max_bp)\n",
    "#                     token_map[max_bp] += 1\n",
    "#                     token_map[pair[0]] -= 1\n",
    "#                     token_map[pair[1]] -= 1\n",
    "#                     cont_flag = 1\n",
    "#                     continue\n",
    "\n",
    "#             # else add character\n",
    "#             temp_token.append(token[i])\n",
    "#             token_map[token[i]] += 1\n",
    "            \n",
    "#         updated_lst.append(temp_token)\n",
    "\n",
    "#     # set tokens to updated_lst of merged token\n",
    "#     tokens = updated_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set = sorted(list(set([token_key for token_key in token_map if token_map[token_key] > -1])))\n",
    "# print(token_set)\n",
    "\n",
    "# # save token_set to file\n",
    "# with open(\"token_set_scifi.txt\", \"w\") as f:\n",
    "#     f.write(\"±\".join(token_set))\n",
    "\n",
    "# load token_set from file\n",
    "with open(\"token_set_scifi.txt\", \"r\") as f:\n",
    "    token_set = f.read().split(\"±\")\n",
    "\n",
    "# print(token_set)\n",
    "\n",
    "# from itertools import chain\n",
    "\n",
    "# bpe_text_in = list(chain.from_iterable(updated_lst))\n",
    "\n",
    "# training_data = open(\"family-guy.txt\", \"r\")\n",
    "# # training_data = training_data.read()\n",
    "# # charset = \"\".join(sorted(set(training_data)))\n",
    "\n",
    "# def encoder(text_in):\n",
    "#     pass\n",
    "            \n",
    "\n",
    "encoder = lambda text_in: [token_set.index(s) for s in text_in]\n",
    "decoder = lambda indices: [token_set[index] for index in indices]\n",
    "# device = \"mps\"\n",
    "\n",
    "# token_set\n",
    "# print(token_map)\n",
    "# token_set[-40:]\n",
    "# encoder(bpe_text_in[])\n",
    "\n",
    "# encoder(\"hello\")\n",
    "# print(bpe_text_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "# context window of 380 tokens\n",
    "block_size = 1024\n",
    "batch_size = 20\n",
    "vocab_size = len(token_set)\n",
    "dropout = 0.2\n",
    "n_heads = 8\n",
    "n_embed = 512\n",
    "num_blocks = 6\n",
    "n_steps = 1200\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "\n",
    "# data = torch.tensor(encoder(bpe_text_in), dtype=torch.long)\n",
    "# save tensor to file \n",
    "# torch.save(data, 'data_scifi.pt')\n",
    "# # load tensor from file\n",
    "# data = torch.load('data_scifi.pt')\n",
    "data = torch.load(\"data_scifi.pt\")\n",
    "data = data.to(device)\n",
    "\n",
    "n = int(0.5*len(data))\n",
    "training_set = data[:n]\n",
    "validation_set = data[n:]\n",
    "\n",
    "\n",
    "def generate_batch():\n",
    "        randlst = torch.randint(len(training_set) - block_size, (batch_size,))#.to(device=\"mps\")\n",
    "        batch = torch.stack([training_set[i:i+block_size] for i in randlst])#.to(device=\"mps\")\n",
    "        targets = torch.stack([training_set[i+1: i+block_size+1] for i in randlst])#.to(device=\"mps\")\n",
    "        return batch,targets\n",
    "\n",
    "def generate_batch_validation():\n",
    "        randlst = torch.randint(len(validation_set) - block_size, (batch_size,))#.to(device=\"mps\")\n",
    "        batch = torch.stack([validation_set[i:i+block_size] for i in randlst])#.to(device=\"mps\")\n",
    "        targets = torch.stack([validation_set[i+1: i+block_size+1] for i in randlst])#.to(device=\"mps\")\n",
    "        return batch,targets\n",
    "\n",
    "# x,y = generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlashMetal import FlashAttentionForward, FlashAttentionBackward, fetchPipeline\n",
    "\n",
    "# forward_pipeline, back_pipeline = fetchPipeline()\n",
    "\n",
    "class FlashAttentionAutograd(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, query, key, value):\n",
    "        batch_size, num_heads, N_seq, n_embed = query.shape\n",
    "\n",
    "        out = torch.zeros_like(value, requires_grad=True, device='mps', dtype=torch.float32).contiguous()\n",
    "        row_max = torch.zeros((batch_size, num_heads, N_seq), device='mps', dtype=torch.float32).contiguous()\n",
    "        row_sum = torch.zeros((batch_size, num_heads, N_seq), device='mps', dtype=torch.float32).contiguous()\n",
    "\n",
    "        # out = FlashAttentionForward(query, key, value, out, row_max, row_sum)#, forward_pipeline)\n",
    "        out, row_max, row_sum = FlashAttentionForward(query, key, value, out, row_max, row_sum )        \n",
    "        ctx.save_for_backward(query, key, value, out, row_max, row_sum)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        query, key, value, out, row_max, row_sum = ctx.saved_tensors\n",
    "        out_dQ = torch.zeros_like(query, device='mps', dtype=torch.float32).contiguous()\n",
    "        out_dK = torch.zeros_like(key, device='mps', dtype=torch.float32).contiguous()\n",
    "        out_dV = torch.zeros_like(value, device='mps', dtype=torch.float32).contiguous()\n",
    "        res_metal = FlashAttentionBackward(query, key, value, out, grad_output.contiguous(), out_dQ, out_dK, out_dV, row_sum, row_max)\n",
    "        grad_query, grad_key, grad_value = res_metal\n",
    "        return grad_query, grad_key, grad_value\n",
    "\n",
    "        # s = query @ key.transpose(-1,-2)\n",
    "        # s /= np.sqrt(16)\n",
    "\n",
    "        # mask = torch.tril(torch.ones_like(s)).to(\"mps\")\n",
    "        # s_masked = torch.where(mask == 1, s, torch.tensor(float('-inf')).to(\"mps\"))\n",
    "\n",
    "        # P = F.softmax(s_masked, -1)\n",
    "\n",
    "        # dV = torch.matmul(P.transpose(-1, -2), grad_output)\n",
    "\n",
    "        # dP = torch.matmul(grad_output, value.transpose(-1, -2))\n",
    "        # dS = P * (dP - torch.sum(dP * P, dim=-1, keepdim=True))\n",
    "\n",
    "        # dQ = (torch.matmul(dS, key))\n",
    "        # dK = torch.matmul(dS.transpose(-1, -2), query)\n",
    "        # return dQ, dK, dV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MHAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.head_size = 16\n",
    "        self.batch_qkv_matrices = nn.Linear(n_embed, self.head_size * n_heads * 3, bias=False)\n",
    "        self.projection = nn.Linear(self.head_size * n_heads, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.kv_cache = None\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        q,k,v = self.batch_qkv_matrices(x).split(self.head_size * n_heads, dim=-1) # Now Q,K,V of dim B, T, head size * n_heads\n",
    "\n",
    "        T_Q = T\n",
    "        T_KV = T\n",
    "\n",
    "        if not self.training:\n",
    "            if self.kv_cache is not None: \n",
    "                # concat new kv onto old kv cache -> update cache\n",
    "                k_prev,v_prev = self.kv_cache\n",
    "                # reset cache if block size reached\n",
    "                if self.kv_cache[0].shape[-2] < block_size:\n",
    "                    k = torch.cat((k_prev, k), dim=1)\n",
    "                    v = torch.cat((v_prev, v), dim=1)\n",
    "                    # trim key-value cache to block size\n",
    "                    # k = k[:, -block_size:, :]\n",
    "                    # v = v[:, -block_size:, :]\n",
    "                    # set T_Q to 1 as we have 1-d query\n",
    "                T_Q = 1\n",
    "            T_KV = k.shape[-2]            \n",
    "            self.kv_cache = (k,v)\n",
    "\n",
    "\n",
    "        q = q.view(B, T_Q, n_heads, self.head_size).transpose(1,2).contiguous() # Now of shape B, n_heads, T, head_size for BMM\n",
    "        k = k.view(B, T_KV, n_heads, self.head_size).transpose(1,2).contiguous()\n",
    "        v = v.view(B, T_KV, n_heads, self.head_size).transpose(1,2).contiguous()\n",
    "\n",
    "\n",
    "        if self.training:\n",
    "            out = FlashAttentionAutograd.apply(q,k,v)\n",
    "            out = out.transpose(1,2) # B, n_heads, T, C --> B, T, n_heads, C   \n",
    "            out = out.contiguous().view(B,T, n_heads * self.head_size)\n",
    "            return self.dropout(self.projection(out))\n",
    "\n",
    "        # for inference\n",
    "        weight_mat = q @ k.transpose(-2, -1)\n",
    "        weight_mat = weight_mat * (self.head_size ** -0.5) \n",
    "        weight_mat = F.softmax(weight_mat, dim=-1)\n",
    "        # Multiply with values\n",
    "        res = weight_mat @ v\n",
    "        res = res.transpose(1,2) # B, n_heads, T, C --> B, T, n_heads, C   \n",
    "        res = res.contiguous().view(B, T, n_heads*self.head_size)\n",
    "        return self.projection(res)\n",
    "\n",
    " \n",
    "     \n",
    "\n",
    "# class Head(nn.Module):\n",
    "#     def __init__(self, head_size):\n",
    "#         super().__init__()\n",
    "#         # print(head_size)\n",
    "#         self.head_size = head_size\n",
    "#         self.batch_qkv_matrices = nn.Linear(n_embed, head_size * n_heads * 3, bias=False) \n",
    "#         self.register_buffer(\"tril\", torch.tril(torch.ones((block_size, block_size))))\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.kv_cache = None\n",
    "    \n",
    "#     # def reset_cache(self):\n",
    "#     #     self.kv_cache = None\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     # initially tensor of B,T,C\n",
    "    #     q,k,v = self.batch_qkv_matrices(x).split(self.head_size * n_heads, dim=-1) # Now Q,K,V of dim B, T, head size * n_heads\n",
    "        \n",
    "    #     B,T,C = x.shape\n",
    "    #     # # assert C % n_heads == 0\n",
    "    #     T_Q = T\n",
    "    #     T_KV = T\n",
    "    #     # if model in inference (eval) mode then use cache\n",
    "        # if not self.training:\n",
    "        #     if self.kv_cache is not None: \n",
    "        #         # concat new kv onto old kv cache -> update cache\n",
    "        #         k_prev,v_prev = self.kv_cache\n",
    "        #         # reset cache if block size reached\n",
    "        #         if self.kv_cache[0].shape[-2] < block_size:\n",
    "        #             k = torch.cat((k_prev, k), dim=1)\n",
    "        #             v = torch.cat((v_prev, v), dim=1)\n",
    "        #             # trim key-value cache to block size\n",
    "        #             # k = k[:, -block_size:, :]\n",
    "        #             # v = v[:, -block_size:, :]\n",
    "\n",
    "        #         # set T_Q to 1 as we have 1-d query\n",
    "        #         T_Q = 1\n",
    "            \n",
    "        #     T_KV = k.shape[-2]            \n",
    "        #     self.kv_cache = (k,v)\n",
    "\n",
    " \n",
    "#         k = k.view(B, T_KV, n_heads, self.head_size).transpose(1,2)\n",
    "#         # # print(f\"K: {k.shape}\")\n",
    "#         q = q.view(B, T_Q, n_heads, self.head_size).transpose(1,2) # Now of shape B, n_heads, T, head_size for BMM\n",
    "#         v = v.view(B, T_KV, n_heads, self.head_size).transpose(1,2)\n",
    "   \n",
    "#         weight_mat = q @ k.transpose(-2, -1)\n",
    "\n",
    "#         weight_mat = weight_mat * (self.head_size ** -0.5) #\n",
    "\n",
    "#         if self.training:\n",
    "#             weight_mat = weight_mat.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "\n",
    "#         weight_mat = F.softmax(weight_mat, dim=-1)\n",
    "#         # # regularisation prevent overfitting\n",
    "#         weight_mat = self.dropout(weight_mat)\n",
    "#         # Multiply with values\n",
    "#         res = weight_mat @ v\n",
    "#         res = res.transpose(1,2) # B, n_heads, T, C --> B, T, n_heads, C   \n",
    "#         res = res.contiguous().view(B, T, self.head_size*n_heads)\n",
    "#         return res\n",
    "    \n",
    "    # COMMENTED OUT FOR TESTING\n",
    "\n",
    "\n",
    "# class MHAttention(nn.Module):\n",
    "#     def __init__(self, num_heads, head_size):\n",
    "#         super().__init__()\n",
    "#         self.att_heads = Head(head_size=head_size) #nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "#         self.projection = nn.Linear(head_size*n_heads, n_embed)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#     def forward(self, x):\n",
    "#         # print(x.shape)\n",
    "#         res = self.att_heads(x)\n",
    "#         # res = torch.cat([att_head(x) for att_head in self.att_heads], dim=-1)\n",
    "#         res = self.dropout(self.projection(res))\n",
    "#         return res \n",
    "    \n",
    "\n",
    "# hd = Head(96).to(\"mps\")\n",
    "# res = hd(torch.randn([64, 1024, 96*16], device=\"mps\"))\n",
    "\n",
    "# mh = MHAttention().to(\"mps\")\n",
    "# x = torch.randn(1,1024,n_embed, device='mps')\n",
    "# loss = torch.mean(out)\n",
    "# # # print(out.shape)\n",
    "# # loss = torch.mean(out)\n",
    "# loss.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, n_embed) -> None:\n",
    "        super().__init__()\n",
    "        scale_factor = 4\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed * scale_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed * scale_factor, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.ff = Feedforward(n_embed)\n",
    "        # self.mhatt = MHAttention(n_heads, 16)\n",
    "        self.mhatt = MHAttention().to(device)\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embed) \n",
    "        self.layer_norm2 = nn.LayerNorm(n_embed)\n",
    "    def forward(self, x):\n",
    "        x = x + self.mhatt(self.layer_norm1(x))\n",
    "        x = x + self.ff(self.layer_norm2(x))\n",
    "        return x \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "       \n",
    "        # self.att = Head(n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_heads) for _ in range(num_blocks)])\n",
    "\n",
    "        self.layernorm =  nn.LayerNorm(n_embed)\n",
    "        \n",
    "    def forward(self, data, target=None, time_inf=None):\n",
    "        # print(data.shape)\n",
    "        token_layer = self.embedding_table(data)\n",
    "        # print(token_layer)\n",
    "        # print(token_layer.shape)\n",
    "        B,T = data.shape\n",
    "\n",
    "\n",
    "        if time_inf != None and T == 1:\n",
    "            # print(torch.ones(1, dtype=torch.long) * time_inf)\n",
    "            # pos_embed arguement must be tensor, must be single value of time_inf \n",
    "            pos_embed = self.pos_embedding_table(torch.ones(1, dtype=torch.long).to(device) * time_inf)\n",
    "        else:\n",
    "            # print(torch.arange(T))\n",
    "            pos_embed = self.pos_embedding_table(torch.arange(T).to(device))\n",
    "        \n",
    "        total = token_layer + pos_embed\n",
    "        # print(f\"INPUT {total.shape}\")\n",
    "\n",
    "        total = self.layernorm(self.blocks(total))\n",
    "        logits = self.embedding_table(total)\n",
    "\n",
    "        if target != None:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C) \n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "            return logits, loss\n",
    "        return logits, None\n",
    "        \n",
    "\n",
    "    def predict(self, data, temperature=1, num_tokens=350):\n",
    "        prev_token = data\n",
    "        for i in range(num_tokens):\n",
    "            t_curr = (i+data.shape[-1]-1) % block_size\n",
    "            logits,lossNone = self(prev_token, time_inf=t_curr)\n",
    "            logits = logits[:,-1,:]\n",
    "            logits = logits / temperature\n",
    "            prob_dist = F.softmax(logits, dim=-1)\n",
    "            prev_token = torch.multinomial(prob_dist, num_samples=1)\n",
    "            # print(prev_token)\n",
    "            print(decoder(prev_token[0].tolist())[0],end=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1200 [00:05<1:49:46,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.8558117151260376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 201/1200 [13:19<1:05:29,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.7938086986541748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 401/1200 [26:26<52:38,  3.95s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.838022232055664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 601/1200 [39:44<40:43,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.8327362537384033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 801/1200 [52:59<26:24,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.788804292678833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1001/1200 [1:06:10<13:06,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.7808431386947632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [1:19:14<00:00,  3.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "from tqdm import tqdm\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "gpt = GPT().to(\"mps\")\n",
    "gpt.load_state_dict(torch.load(\"scifi_model_checkpoint.pth\"))\n",
    "gpt.train()\n",
    "optimiser = torch.optim.AdamW(gpt.parameters(), lr=6e-4, betas=(0.9, 0.95), eps=1e-8, weight_decay=0.1)\n",
    "for steps in tqdm(range(n_steps)):\n",
    "    batch,target = generate_batch()\n",
    "    logits, loss = gpt(batch,target)\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward() \n",
    "    torch.nn.utils.clip_grad_norm_(gpt.parameters(), 1.0)\n",
    "    optimiser.step()\n",
    "\n",
    "    if steps % 200 == 0:\n",
    "        # print(gpt.blocks[0].mhatt.batch_qkv_matrices.weight)\n",
    "    #     # validation_batch, validation_target = generate_batch_validation()\n",
    "    #     # validation_logits, validation_loss = gpt(validation_batch,validation_target)\n",
    "        print(f\"Training Loss: {loss.item()}\")#. Validation Loss: {validation_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text...\n",
      "\"Hello\", said John -- \" \"We won't be men, I'm really very ready to best diseased by all the young Yorkers, You're not adapted of that name could be done. I woke without life and Buckkett and I began to know you can't telling the upper woman. I was far a straightium, Jhoh. Out of my attempts, heavi took the last suit on the crow, I'd have to go there, and I've come up impression to him, but I judged us and smiled brisks a large of barely occurred and saw the time I also bjected the head of side the surface with them got the Mlayman's last of antisn't a city... His little little ship dot of mass, and monitor but the alien did not have a agilea while by starting the hoverrobot, this is trying to plot the avalue-rob"
     ]
    }
   ],
   "source": [
    "# torch.save(gpt.state_dict(), \"scifi_model_checkpoint.pth\")\n",
    "\n",
    "def get_prompt(input_str):\n",
    "    lst = []\n",
    "    # we are iterating through each character\n",
    "    # for each character, we check if it + curr_token is in token_set\n",
    "    # if it is, then we continue \n",
    "    curr_tok = input_str[0]\n",
    "    for i in range(1, len(input_str)):\n",
    "        if (curr_tok + input_str[i]) not in token_set:\n",
    "            lst.append(curr_tok)\n",
    "            curr_tok = input_str[i]\n",
    "            continue\n",
    "        else:\n",
    "            curr_tok += input_str[i]\n",
    "\n",
    "    lst.append(curr_tok)\n",
    "    return lst\n",
    "\n",
    "\n",
    "gpt_eval = GPT().to(device)       \n",
    "gpt_eval.load_state_dict(torch.load(\"scifi_model_checkpoint.pth\"))\n",
    "\n",
    "prompt = \"\\\"Hello\\\", said John \"\n",
    "encoded_prompt = encoder(get_prompt(prompt))\n",
    "\n",
    "test = torch.tensor(encoded_prompt ,dtype=torch.long, device=device)\n",
    "# # # print(test)\n",
    "# # # # # # # print(test.shape)\n",
    "K = 1\n",
    "test = test.unsqueeze(0).repeat(K, 1)\n",
    "# # # new_model = gpt.to(device)\n",
    "# # # new_model.load_state_dict(torch.load(\"family-guy-lm-BPE-C380-E80-N9000-H7.pth\"))\n",
    "# # new_model.eval()\n",
    "\n",
    "gpt_eval.eval()\n",
    "with torch.no_grad():\n",
    "    print(\"Generating text...\")\n",
    "    print(prompt, end=\"\")\n",
    "    out = gpt_eval.predict(test, num_tokens=500, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Peter: Yeah.\n",
    "#   Peter: Where do you think you are? What do you think you're thinking?\n",
    "#   Lois: I don't know. I'm kidding. He didn't even know you how to explain it\n",
    "#   was bad. It's a time machine.\n",
    "#   Stewie: Of course! Did you see my dunk was at the other room?\n",
    "#   Peter: Let's see. He died in!\n",
    "#   Stewie: I swear to God, he didn't know he's dead!\n",
    "#   Peter: Great, huh? Can I see your dad stinks? He found the dad gets\n",
    "#   firest, Lois, but we gotta be him for a family after lunch with the\n",
    "#   filler. Here comes the White Lagmie Prince things]\n",
    "# [closing theme music]\n",
    "\n",
    "# Title: Da Me Life Over\n",
    "\n",
    "# Theme Song\n",
    "#   Lois: Oh, I'm sorry. They have to face a little further\n",
    "#   reason I call a responsibility to jai\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # import torch\n",
    "# # # from FlashMetal import FlashAttentionMPS\n",
    "\n",
    "# q = torch.randn(batch_size, n_heads, block_size, 96, requires_grad=True, device=\"mps\")\n",
    "# k = torch.randn(batch_size, n_heads, block_size, 96, requires_grad=True, device=\"mps\")\n",
    "# v = torch.randn(batch_size, n_heads, block_size, 96, requires_grad=True, device=\"mps\")\n",
    "\n",
    "# out = FlashAttentionAutograd.apply(q,k,v)\n",
    "# out.retain_grad()\n",
    "# # loss\n",
    "# l1 = torch.mean(out)\n",
    "# l1.backward()\n",
    "\n",
    "\n",
    "# dO_test = (out.grad)\n",
    "\n",
    "# s = q @ k.transpose(-1,-2)\n",
    "# s /= np.sqrt(96)\n",
    "\n",
    "# mask = torch.tril(torch.ones_like(s)).to(\"mps\")\n",
    "# s_masked = torch.where(mask == 1, s, torch.tensor(float('-inf')).to(\"mps\"))\n",
    "\n",
    "# P = F.softmax(s_masked, -1)\n",
    "\n",
    "# o_test = (torch.matmul(P, v))\n",
    "\n",
    "# #dO = torch.randn_like(q, device='mps')\n",
    "# o1 = (s_masked @ v)\n",
    "# dP = torch.matmul(dO_test, v.transpose(-1, -2))\n",
    "# dS = P * (dP - torch.sum(dP * P, dim=-1, keepdim=True))\n",
    "\n",
    "# dQ_test = torch.matmul(dS, k)\n",
    "# print(dQ_test)\n",
    "# print(q.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(gpt.embedding_table(torch.zeros(2, dtype=torch.long, device=\"mps\")))\n",
    "\n",
    "# # print(gpt.blocks[0].mhatt.batch_qkv_matrices.weight)\n",
    "\n",
    "# q = torch.randn(1, 1, block_size, 96, requires_grad=True, device=\"mps\")\n",
    "# k = torch.randn(1, 1, block_size, 96, requires_grad=True, device=\"mps\")\n",
    "# v = torch.randn(1, 1, block_size, 96, requires_grad=True, device=\"mps\")\n",
    "\n",
    "# # # # # print(v.is_contiguous())\n",
    "\n",
    "\n",
    "\n",
    "# # # for i in range(1):\n",
    "# o = FlashAttentionAutograd.apply(q,k,v)\n",
    "\n",
    "# # attention\n",
    "# s = q @ k.transpose(-1,-2)\n",
    "# s /= np.sqrt(96)\n",
    "# # apply mask\n",
    "# mask = torch.tril(torch.ones_like(s)).to(\"mps\")\n",
    "# s_masked = torch.where(mask == 1, s, torch.tensor(float('-inf')).to(\"mps\"))\n",
    "\n",
    "# max_vals = torch.max(s, dim=-1)[0]\n",
    "\n",
    "# # exp_att = torch.exp(s_masked - max_vals)\n",
    "# # sum_exp_att = torch.sum(exp_att, dim=-1)\n",
    "# # print(max_vals)\n",
    "\n",
    "# P = F.softmax(s_masked, -1)\n",
    "# # output\n",
    "# o2 = (torch.matmul(P, v))\n",
    "\n",
    "\n",
    "# # print(sum_exp_att.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.0482,  1.3399,  0.0039,  ..., -0.2387, -0.1393, -1.8804],\n",
       "          [-0.1157,  0.5917, -1.0879,  ..., -0.1437,  1.6654, -1.7925],\n",
       "          [ 0.0839, -0.0993, -1.2655,  ...,  0.9549,  1.1826, -1.1281],\n",
       "          ...,\n",
       "          [-0.7517, -0.5227, -0.0832,  ...,  2.9015, -2.9129, -0.1119],\n",
       "          [ 0.5305, -1.0390, -2.2558,  ...,  2.3955,  3.4465, -3.6645],\n",
       "          [-0.2959, -0.2837, -1.1684,  ...,  1.5058,  2.0139, -1.0723]]],\n",
       "        device='mps:0'),\n",
       " tensor([[[-0.2176,  2.2212,  3.0070,  ...,  0.6627, -1.8530,  0.0553],\n",
       "          [-0.5412, -0.5362,  1.1293,  ...,  0.6604, -0.0362, -0.6014],\n",
       "          [-1.3144,  0.9443,  2.0863,  ...,  1.1473,  1.3490,  3.3220],\n",
       "          ...,\n",
       "          [-0.8646,  1.9807,  0.4855,  ...,  0.2019, -1.4975, -0.8897],\n",
       "          [-0.4465, -0.0870,  0.6601,  ...,  0.8354,  1.3764,  2.4056],\n",
       "          [-0.3186,  0.3021,  0.0438,  ..., -1.4146,  0.4388, -0.7059]]],\n",
       "        device='mps:0'))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
